setwd("~/Documents/Machinelearning/code")
# Build model of species diversity
data <- expand.grid(temperature=seq(0,40,4), humidity=seq(0,100,10),
carbon=seq(1,10,1), herbivores=seq(0,20,2))
data$plants <- runif(nrow(data), 3, 5)
data$plants <- with(data, plants + temperature * .1)
data$plants[data$humidity > 50] <- with(data[data$humidity > 50,],
plants + humidity * .05)
data$plants[data$carbon < 2] <- with(data[data$carbon < 2,], plants - carbon)
data$plants <- with(data, plants + herbivores * .1)
data$plants[data$herbivores > 5 & data$herbivores < 15] <-
with(data[data$herbivores > 5 & data$herbivores < 15,], plants - herbivores * .2)
# Draw random data from Poisson based on this
for(i in seq_len(nrow(data)))
data$plants[i] <- rpois(1, data$plants[i])
library(tree)
# Pick some training data and then fit a model to it
training <- sample(nrow(data), nrow(data)/2)
model <- tree(plants~., data=data[training,])
# Examine the model
plot(model)
text(model)
# Look at the statistics of the model
model
summary(model)
install.packages("tree")
library(tree)
# Pick some training data and then fit a model to it
training <- sample(nrow(data), nrow(data)/2)
model <- tree(plants~., data=data[training,])
# Examine the model
plot(model)
text(model)
library(tree)
# Pick some training data and then fit a model to it
training <- sample(nrow(data), nrow(data)/2)
model <- tree(plants~., data=data[training,])
# Examine the model
plot(model)
text(model)
# Look at the statistics of the model
model
summary(model)
# Check performance outside training set
cor.test(predict(model, data[-training,]), data$plants[-training])
# Check cross-validation of model
plot(cv.tree(model))
# Check performance outside training set
cor.test(predict(model, data[-training,]), data$plants[-training])
text(model)
model <- randomForest(plants~., data=data[training,], mtry=ncol(data)-1)
library(randomForest)
library(randomForest)
model <- randomForest(plants~., data=data[training,], mtry=ncol(data)-1)
cor.test(predict(model, data[-training,]), data$plants[-training])
cor.test(predict(model, data[-training,]), data$plants[-training])
model <- randomForest(plants~., data=data[training,], mtry=ncol(data)-1)
cor.test(predict(model, data[-training,]), data$plants[-training])
model <- randomForest(plants~., data=data[training,], importance=TRUE)
importance(model)
cor.test(predict(model, data[-training,]), data$plants[-training])
install.packages("gbm")
library(gbm)
# Fit a boosted regression tree model to Poisson (count) data
model <- gbm(
plants ~ .,                 # Response ~ all predictors
data = data[training, ],    # Use training subset
distribution = "poisson"    # Specify Poisson for count data
)
summary(model)
# Fit a second model with a larger shrinkage (=0.1),
# which places more weight on the later trees in the boosting sequence.
faster.model <- gbm(
plants ~ .,
data = data[training, ],
distribution = "poisson",
shrinkage = 0.1
)
# Fit a second model with a larger shrinkage (=0.1),
# which places more weight on the later trees in the boosting sequence.
faster.model <- gbm(
plants ~ .,
data = data[training, ],
distribution = "poisson",
shrinkage = 0.05
)
# Fit a second model with a larger shrinkage (=0.1),
# which places more weight on the later trees in the boosting sequence.
faster.model <- gbm(
plants ~ .,
data = data[training, ],
distribution = "poisson",
shrinkage = 0.1
)
summary(faster.model)
# Fit a second model with a larger shrinkage (=0.1),
# which places more weight on the later trees in the boosting sequence.
faster.model <- gbm(
plants ~ .,
data = data[training, ],
distribution = "poisson",
shrinkage = 0.05
)
summary(faster.model)
# Fit a second model with a larger shrinkage (=0.1),
# which places more weight on the later trees in the boosting sequence.
faster.model <- gbm(
plants ~ .,
data = data[training, ],
distribution = "poisson",
shrinkage = 0.01
)
summary(faster.model)
# Fit a second model with a larger shrinkage (=0.1),
# which places more weight on the later trees in the boosting sequence.
faster.model <- gbm(
plants ~ .,
data = data[training, ],
distribution = "poisson",
shrinkage = 0.1
)
summary(faster.model)
explanatory <- replicate(1000, rnorm(1000))
response <- explanatory[,123]*1.5 -explanatory[,678]*.5
install.packages("lars")
explanatory <- replicate(1000, rnorm(1000))
response <- explanatory[,123]*1.5 -explanatory[,678]*.5
model <- lars(explanatory, response, type="lasso")
plot(model)
library(lars)
model <- lars(explanatory, response, type="lasso")
plot(model)
model <- lars(explanatory, response, type="lasso")
plot(model)
signif.coefs <- function(model, threshold=0.001){
coefs <- coef(model)
signif <- which(abs(coefs[nrow(coefs),]) > threshold)
return(setNames(coefs[nrow(coefs),signif], signif))
}
signif.coefs(model)
explanatory <- replicate(1000, rnorm(1000))
response <- explanatory[,123]*1.5 -explanatory[,678]*.5
model <- lars(explanatory, response, type="lasso")
plot(model)
signif.coefs <- function(model, threshold=0.001){
coefs <- coef(model)
signif <- which(abs(coefs[nrow(coefs),]) > threshold)
return(setNames(coefs[nrow(coefs),signif], signif))
}
signif.coefs(model)
# 1. Fit a model using Least Angle Regression (LAR) instead of Lasso.
#    'type="lar"' indicates we're using the LAR algorithm rather than the Lasso one.
model <- lars(explanatory, response, type = "lar")
# 2. Plot the LAR coefficient path. Similar to the Lasso plot, it shows
#    how each variable's coefficient changes as the penalty is relaxed.
plot(model)
# 3. Apply our previously defined function to extract the "significant"
#    final coefficients from the LAR model. Because the data was simulated
#    with two truly important variables, we expect to see them here.
signif.coefs(model)
# 1. Fit a model using Least Angle Regression (LAR) instead of Lasso.
#    'type="lar"' indicates we're using the LAR algorithm rather than the Lasso one.
model <- lars(explanatory, response, type = "lar")
# 2. Plot the LAR coefficient path. Similar to the Lasso plot, it shows
#    how each variable's coefficient changes as the penalty is relaxed.
plot(model)
# 3. Apply our previously defined function to extract the "significant"
#    final coefficients from the LAR model. Because the data was simulated
#    with two truly important variables, we expect to see them here.
signif.coefs(model)
# 4. Fit a "bad" LAR model with normalization turned off.
#    Because our 'explanatory' data is standard normal, we wouldn't
#    usually see a big problem here. But let's see how it behaves.
bad.model <- lars(explanatory, response, type = "lar", normalize = FALSE)
# 5. Extract coefficients with threshold = 0, forcing the function to show
#    every coefficient, including tiny ones. Notice that many more variables
#    might appear with non-zero coefficients.
signif.coefs(bad.model, thresh = 0)
# 6. Compare with the "good" model that uses the default normalization.
#    Because variables are normalized here, we don't see a bunch of tiny,
#    spurious coefficients.
signif.coefs(model, thresh = 0)
# Conclusion:
# Conclusion:
#   * When variables differ widely in scale, turning off normalization can
# Conclusion:
#   * When variables differ widely in scale, turning off normalization can
#     lead to misleading results (huge or tiny coefficients in the final model).
# 5. Extract coefficients with threshold = 0, forcing the function to show
#    every coefficient, including tiny ones. Notice that many more variables
#    might appear with non-zero coefficients.
signif.coefs(bad.model, thresh = 0)
# 6. Compare with the "good" model that uses the default normalization.
#    Because variables are normalized here, we don't see a bunch of tiny,
#    spurious coefficients.
signif.coefs(model, thresh = 0)
# 1. Fit a model using Least Angle Regression (LAR) instead of Lasso.
#    'type="lar"' indicates we're using the LAR algorithm rather than the Lasso one.
model <- lars(explanatory, response, type = "lar")
# 2. Plot the LAR coefficient path. Similar to the Lasso plot, it shows
#    how each variable's coefficient changes as the penalty is relaxed.
plot(model)
# 2. Plot the LAR coefficient path. Similar to the Lasso plot, it shows
#    how each variable's coefficient changes as the penalty is relaxed.
plot(model)
# 3. Apply our previously defined function to extract the "significant"
#    final coefficients from the LAR model. Because the data was simulated
#    with two truly important variables, we expect to see them here.
signif.coefs(model)
# 4. Fit a "bad" LAR model with normalization turned off.
#    Because our 'explanatory' data is standard normal, we wouldn't
#    usually see a big problem here. But let's see how it behaves.
bad.model <- lars(explanatory, response, type = "lar", normalize = FALSE)
# 5. Extract coefficients with threshold = 0, forcing the function to show
#    every coefficient, including tiny ones. Notice that many more variables
#    might appear with non-zero coefficients.
signif.coefs(bad.model, thresh = 0)
# 6. Compare with the "good" model that uses the default normalization.
#    Because variables are normalized here, we don't see a bunch of tiny,
#    spurious coefficients.
signif.coefs(model, thresh = 0)
# Conclusion:
# Conclusion:
#   * When variables differ widely in scale, turning off normalization can
# Conclusion:
#   * When variables differ widely in scale, turning off normalization can
#     lead to misleading results (huge or tiny coefficients in the final model).
# Conclusion:
#   * When variables differ widely in scale, turning off normalization can
#     lead to misleading results (huge or tiny coefficients in the final model).
#   * Normalizing (scaling) variables is generally good practice before
#1. Create a data frame with 2 random variables:
#    * replicate(2, rnorm(1000)) generates 2 columns (x, X2),
#      each with 1000 random normal values.
data <- data.frame(replicate(2, rnorm(1000)))
# 2. Add a new column 'y' based on whether the sum of the two random columns
#    falls within a band around the median sum.
#    * rowSums(data) calculates x + X2 for each row.
#    * median(rowSums(data)) finds the median of those sums.
#    * We define 'y' as TRUE if the row sum is within ±1 of this median,
#      and FALSE otherwise.
data$y <- (rowSums(data) > (median(rowSums(data)) - 1)) &
(rowSums(data) < (median(rowSums(data)) + 1))
# 3. Plot the first column of the data (named 'x') against its row index,
#    using different colors to indicate whether 'y' is TRUE or FALSE.
#    * pch=20 uses small solid dots.
#    * ifelse(y, "red", "black") sets point color to red for TRUE and black for FALSE.
with(data, plot(x,
pch = 20,
col = ifelse(y, "red", "black")))
# 1. Create a data frame with 2 random variables:
#    * replicate(2, rnorm(1000)) generates 2 columns (x, X2),
#      each with 1000 random normal values.
data <- data.frame(replicate(2, rnorm(1000)))
# 2. Add a new column 'y' based on whether the sum of the two random columns
#    falls within a band around the median sum.
#    * rowSums(data) calculates x + X2 for each row.
#    * median(rowSums(data)) finds the median of those sums.
#    * We define 'y' as TRUE if the row sum is within ±1 of this median,
#      and FALSE otherwise.
data$y <- (rowSums(data) > (median(rowSums(data)) - 1)) &
(rowSums(data) < (median(rowSums(data)) + 1))
# 3. Plot the first column of the data (named 'x') against its row index,
#    using different colors to indicate whether 'y' is TRUE or FALSE.
#    * pch=20 uses small solid dots.
#    * ifelse(y, "red", "black") sets point color to red for TRUE and black for FALSE.
with(data, plot(x,
pch = 20,
col = ifelse(y, "red", "black")))
data <- data.frame(replicate(2, rnorm(1000)))
data$y = (rowSums(data) > (median(rowSums(data)) - 1)) &
(rowSums(data) < (median(rowSums(data)) + 1))
with(data, plot(x, pch=20, col=ifelse(y, "red", "black")))
install.packages("e1071")
training <- sample(nrow(data), nrow(data)/2)
model <- svm(y~., data=data[training,], type="C")
plot(model, data[training,])
#Modules
library(e1071)
training <- sample(nrow(data), nrow(data)/2)
model <- svm(y~., data=data[training,], type="C")
plot(model, data[training,])
table(predict(model, data[-training,]), data$y[-training])
tune.svm(factor(y)~., data=data[-training,], gamma=c(.5,1,10), cost=c(1,10))
x <- rnorm(1000)
y <- -x
data <- data.frame(scale(cbind(x,y)))
library(neuralnet)
model <- neuralnet(y ~ x, hidden=0, data=data)
install.packages("neuralnet")
x <- rnorm(1000)
y <- -x
data <- data.frame(scale(cbind(x,y)))
model <- neuralnet(y ~ x, hidden=0, data=data)
plot(model)
x <- rnorm(1000)
y <- -x
data <- data.frame(scale(cbind(x,y)))
model <- neuralnet(y ~ x, hidden=0, data=data)
plot(model)
library(neuralnet)
model <- neuralnet(y ~ x, hidden=0, data=data)
plot(model)
# Generate 1000 random numbers from a normal distribution
x <- rnorm(1000)
# Create y as the negative of x (perfectly negatively correlated)
y <- -x
# Combine x and y into a data frame and scale both variables (mean = 0, sd = 1)
data <- data.frame(scale(cbind(x, y)))
# Train a neural network with no hidden layers to predict y from x
model <- neuralnet(y ~ x, hidden = 0, data = data)
# Plot the structure of the trained neural network
plot(model)
dist <- dist(biomass)
explanatory <- data.frame(replicate(10, rnorm(400)))
names(explanatory) <- letters[1:10]
response <- with(explanatory, a*2 -0.5*b - i*j + exp(abs(c)))
data <- data.frame(scale(cbind(explanatory,response)))
# Generate a data frame with 10 predictor variables (explanatory variables)
# Each variable is a random normal distribution with 400 observations
explanatory <- data.frame(replicate(10, rnorm(400)))
# Assign column names to the explanatory variables using the first 10 letters of the alphabet
names(explanatory) <- letters[1:10]
# Create the response variable using a mathematical function of some explanatory variables
response <- with(explanatory,
a * 2        # Variable 'a' contributes positively, multiplied by 2
- 0.5 * b    # Variable 'b' has a negative influence, scaled by -0.5
- i * j      # Interaction effect: product of 'i' and 'j' reduces the response
+ exp(abs(c)) # Exponential transformation of absolute 'c', making it always positive
)
# Combine the explanatory variables and response variable into a single data frame
# Scale all variables (mean = 0, standard deviation = 1) to normalize data
data <- data.frame(scale(cbind(explanatory, response)))
training <- sample(nrow(data), nrow(data)/2)
model <- neuralnet(response~a+b+c+d+e+f+g+h+i+j,
dat=data[training,], hidden=5)
cor.test(compute(model, data[-training,1:10])$net.result[,1],
data$response[-training])
plot(model)
library(neuralnet)
# Generate a data frame with 10 predictor variables (explanatory variables)
# Each variable is a random normal distribution with 400 observations
explanatory <- data.frame(replicate(10, rnorm(400)))
# Assign column names to the explanatory variables using the first 10 letters of the alphabet
names(explanatory) <- letters[1:10]
# Create the response variable using a mathematical function of some explanatory variables
response <- with(explanatory,
a * 2        # Variable 'a' contributes positively, multiplied by 2
- 0.5 * b    # Variable 'b' has a negative influence, scaled by -0.5
- i * j      # Interaction effect: product of 'i' and 'j' reduces the response
+ exp(abs(c)) # Exponential transformation of absolute 'c', making it always positive
)
# Combine the explanatory variables and response variable into a single data frame
# Scale all variables (mean = 0, standard deviation = 1) to normalize data
data <- data.frame(scale(cbind(explanatory, response)))
training <- sample(nrow(data), nrow(data)/2)
model <- neuralnet(response~a+b+c+d+e+f+g+h+i+j,
dat=data[training,], hidden=5)
cor.test(compute(model, data[-training,1:10])$net.result[,1],
data$response[-training])
install.packages("NeuralNetTools")
library(NeuralNetTools)
training <- sample(nrow(data), nrow(data)/2)
model <- neuralnet(response~a+b+c+d+e+f+g+h+i+j,
dat=data[training,], hidden=5)
cor.test(compute(model, data[-training,1:10])$net.result[,1],
data$response[-training])
plot(model)
training <- sample(nrow(data), nrow(data)/2)
model <- neuralnet(response~a+b+c+d+e+f+g+h+i+j,
dat=data[training,], hidden=5)
cor.test(compute(model, data[-training,1:10])$net.result[,1],
data$response[-training])
plot(model)
library(NeuralNetTools)
garson(model, bar_plot=FALSE)
garson(model)
garson(model)
garson(model, bar_plot=FALSE)
garson(model)
garson(model)
source("~/Documents/Machinelearning/code/artificialneuralnets.R")
library(NeuralNetTools)
garson(model, bar_plot=FALSE)
garson(model)
